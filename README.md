GPT From Scratch

A minimal, educational implementation of a GPT-style language model written completely from scratch in Python and PyTorch.

This project walks through the core ideas behind modern transformer models, including:
	•	Tokenization and dataset preparation
	•	Self-attention mechanism
	•	Multi-head attention
	•	Feed-forward networks
	•	Positional embeddings
	•	Training a character-level language model
	•	Generating text autoregressively

The goal of this repo is to help learners understand how GPT works internally by building each component step-by-step, inspecting intermediate outputs, and experimenting with the architecture in a simple, transparent codebase.

Suitable for beginners, students, and anyone looking to build a solid technical foundation in modern deep learning and transformer-based language models.
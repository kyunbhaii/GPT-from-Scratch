GPT From Scratch

A minimal, educational implementation of a GPT-style language model written completely from scratch in Python and PyTorch.

This project walks through the core ideas behind modern transformer models, including:
	•	Tokenization and dataset preparation
	•	Self-attention mechanism
	•	Multi-head attention
	•	Feed-forward networks
	•	Positional embeddings
	•	Training a character-level language model
	•	Generating text autoregressively

The goal of this repo is to help learners understand how GPT works internally by building each component step-by-step, inspecting intermediate outputs, and experimenting with the architecture in a simple, transparent codebase.

Suitable for beginners, students, and anyone looking to build a solid technical foundation in modern deep learning and transformer-based language models.

Reference Video

This project follows the implementation demonstrated by Andrej Karpathy in his YouTube lecture: [Karpathy YT Lecture](https://www.youtube.com/watch?v=kCc8FmEb1nY)￼

Watching the video will give you a clearer understanding of each step in the model. This repo includes some additional files to make the learning process easier and more interactive.
